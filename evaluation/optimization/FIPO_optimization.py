import argparse
import torch
import os
os.environ['HF_HOME'] = '/nas/user77/workspace/models'
os.environ['HF_CACHE'] = '/nas/user77/workspace/models'
os.environ['TRANSFORMERS_CACHE'] = '/nas/user77/workspace/models'
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import json
import time
from tqdm import tqdm
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


model_path = os.path.expanduser("Junrulu/FIPO-IPL-IPO-Tulu2-70B")
PROMPT_TEMPLATE = """<|user|>
You are an expert of prompt optimization.


```
Silver Prompt:
{}

```

The optional Silver Response was generated by an AI based on the Silver Prompt. Please help modify the Silver Prompt to Golden Prompt that can obtain a more correct response, in reference to the optional Golden Response. The Golden Prompt should be instructive, concise and strictly faithful to any factual information in the Silver Prompt. The length of the Golden Prompt should be less than 512 words. Only give me the Golden Prompt, do not contain any other information (e.g., your response of the Golden Prompt, any postfix like 'Golden Prompt', no reason why you changed the prompt like, Golden Response, etc.). 

\n<|assistant|>\n
"""
DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'


def optimize_prompt(input_text: str, model, tokenizer, sampling_params):
    """Optimize a single prompt and measure latency"""
    # 먼저 input_text의 길이를 체크하고 필요시 자르기
    max_input_tokens = 3500  # PROMPT_TEMPLATE 오버헤드를 고려하여 여유 공간 확보
    input_tokens = tokenizer.encode(input_text)
    
    if len(input_tokens) > max_input_tokens:
        # 토큰 단위로 자르기 (단어 단위보다 정확)
        truncated_tokens = input_tokens[:max_input_tokens]
        input_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)
    
    prompt = PROMPT_TEMPLATE.format(input_text)
    
    # 최종 프롬프트 길이 확인 및 조정
    t_prompt = tokenizer.encode(prompt)
    if len(t_prompt) > 3900:  # 생성할 토큰을 위한 여유 공간 확보
        # 다시 한번 더 짧게 자르기
        max_input_tokens = 3200
        input_tokens = tokenizer.encode(input_text)
        truncated_tokens = input_tokens[:max_input_tokens]
        input_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        prompt = PROMPT_TEMPLATE.format(input_text)
    
    # Measure optimization time (only model inference)
    start_time = time.time()
    raw_output = model.generate([prompt], sampling_params)
    end_time = time.time()
    latency = end_time - start_time
    
    # Output processing (not included in latency)
    text_output = raw_output[0].outputs[0].text.strip()
    
    # FIPO 형식에 맞게 output 추출
    if '<|assistant|>' in text_output:
        text_output = text_output.split('<|assistant|>')[1].strip()
    
    # "Golden Prompt:" 제거
    resp = text_output.replace("Golden Prompt:", "").strip()
    
    return resp, latency

def main(dataset: str, input_path: str = None, output_path: str = None, load_in_8bit: bool = False):

    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    # 8비트 양자화 설정
    model_kwargs = {
        "model": model_path,
        "gpu_memory_utilization": 0.85,
        "tensor_parallel_size": 1,
        "max_model_len": 4096,
        "max_num_batched_tokens": 4096,
        "trust_remote_code": True
    }
    if args.load_in_8bit:
        model_kwargs["quantization"] = "bitsandbytes"

    model = LLM(**model_kwargs)
    sampling_params = SamplingParams(max_tokens=4096, temperature=0.8, top_p=0.95)
     
    if args.input_path.endswith('.jsonl'):
        data = []
        with open(args.input_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
    else:
        with open(args.input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

    # CausalJudgement 데이터셋의 경우 examples 배열을 사용
    if dataset == "judgement":
        data = data.get("examples", [])

    results = []

    for entry in tqdm(data, desc=f"Processing {dataset}"):

        if dataset == "arena-hard":
            inp = entry.get("conversation", [{}])[0].get("content", "")
        elif dataset == "complex":
            inp = entry.get("instruction_en", "")
        elif dataset == "judgement":
            inp = entry.get("input", "")
        elif dataset == "gsm8k":
            inp = entry.get("question", "")
        else:
            instruction = entry.get("instruction", "")
            if dataset in ['dolly', 'self_instruct'] and 'context' in entry:
                context = entry.get("context", "")
                inp = f"{instruction}\n{context}"
            else:   
                inp = instruction

        try:
            optimized, latency = optimize_prompt(inp, model, tokenizer, sampling_params)
        except Exception as e:
            print(f"[Error] Failed to optimize prompt: {inp[:100]}... → {e}")
            continue

        if dataset == "dolly" or dataset == "self_instruct":
            results.append({
                "input": inp,
                "instruction": instruction,
                "context": context,
                "optimized_prompt": optimized,
                "latency": round(latency, 4)
            })
        elif dataset == "judgement":
            results.append({
                "input": inp,
                "optimized_prompt": optimized,
                "option": "\nOptions:\n- Yes\n- No",
                "latency": round(latency, 4)
            })
        else:
            results.append({
                "input": inp,
                "optimized_prompt": optimized,
                "latency": round(latency, 4)
            })


    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"Done! Results saved to: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimize prompts using a pretrained LLM.")
    # parser.add_argument("--model_name", type=str, required=True, help="Model name (e.g., BPO, PAS)")
    parser.add_argument("--dataset", type=str, required=True, help="Dataset name (e.g., self_instruction)")
    parser.add_argument("--input_path", type=str, default=None, help="(Optional) Manually specify input JSON file")
    parser.add_argument("--output_path", type=str, default=None, help="(Optional) Manually specify output JSON path")
    parser.add_argument("--load_in_8bit", action="store_true", help='Load model in 8-bit quantization')

    args = parser.parse_args()
    main(
        dataset=args.dataset,
        input_path=args.input_path,
        output_path=args.output_path,
        load_in_8bit=args.load_in_8bit
    )

# CUDA_VISIBLE_DEVICES=2 python evaluation/src/FIPO_optimization.py --dataset dolly --input_path evaluation/eval_dataset/dolly.json --output_path evaluation/outputs/FIPO/optimized_prompt/dolly.json --load_in_8bit
# CUDA_VISIBLE_DEVICES=2 python evaluation/src/FIPO_optimization.py --dataset arena-hard --input_path evaluation/eval_dataset/arena-hard_495.json --output_path evaluation/outputs/FIPO/optimized_prompt/arena-hard_495.json --load_in_8bit
# CUDA_VISIBLE_DEVICES=2 python evaluation/src/FIPO_optimization.py --dataset koala --input_path evaluation/eval_dataset/koala.json --output_path evaluation/outputs/FIPO/optimized_prompt/koala.json --load_in_8bit
# CUDA_VISIBLE_DEVICES=2 python evaluation/src/FIPO_optimization.py --dataset self_instruct --input_path evaluation/eval_dataset/self_instruct.json --output_path evaluation/outputs/FIPO/optimized_prompt/self_instruct.json --load_in_8bit